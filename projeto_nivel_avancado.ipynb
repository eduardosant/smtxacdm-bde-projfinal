{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projeto Final de Spark\n",
    "\n",
    "Campanha Nacional de Vacinação contra Covid-19\n",
    "\n",
    "Nível Avançado:\n",
    "Replicar as visualizações do site “https://covid.saude.gov.br/”, porém acessando diretamente a API de Elastic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+-----+\n",
      "|key                                |value|\n",
      "+-----------------------------------+-----+\n",
      "|Covid-19-Coronavac-Sinovac/Butantan|9083 |\n",
      "|Vacina Covid-19 - Covishield       |917  |\n",
      "+-----------------------------------+-----+\n",
      "\n",
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "# Criação da string com a URL de destino da API Elastic para fonte de dados MSCOVIDBR\n",
    "es_url = \"https://imunizacao-es.saude.gov.br/desc-imunizacao/_search\"\n",
    "# Agora vamos ler os dados do arquivo csv com o seguinte parâmetro da API:\n",
    "# {\n",
    "#  \"size\": 10000\n",
    "# }\n",
    "mscovidbrinputDf = spark.read.option('header', 'true').csv('/user/jes/data/mscovidbr_input.csv')\n",
    "# E criamos uma tabela temporária agora usando o 'mscovidbrinputDf'\n",
    "mscovidbrinputDf.createOrReplaceTempView('mscovidbrinputtbl')\n",
    "# Agora criamos o mapa de parâmetros para passar para a fonte de dados REST\n",
    "prmsMSCOVIDBR = { 'url' : es_url, 'input' : 'mscovidbrinputtbl', 'userId' : 'imunizacao_public', 'userPassword' : 'qlto5t&7r_@+#Tlstigi', 'method' : 'GET', 'readTimeout' : '10000', 'connectionTimeout' : '2000', 'partitions' : '10'}\n",
    "# Agora criamos o Dataframe que contém o resultado da chamada para a API com o parâmetro\n",
    "mscovidbrDf = spark.read.format(\"org.apache.dsext.spark.datasource.rest.RestDataSource\").options(**prmsMSCOVIDBR).load()\n",
    "# Nós inspecionamos a estrutura dos resultados retornados.\n",
    "mscovidbrDf.printSchema()\n",
    "# Criamos o Dataframe 'mscovidbrDf_vacina' utilizando a função 'explode' \n",
    "# para retornar uma nova linha para cada elemento no array com dados dos pacientes vacinados.\n",
    "mscovidbrDf_vacina = mscovidbrDf.select(F.explode(F.col(\"output.hits.hits\").alias(\"paciente\")))\n",
    "# Nós inspecionamos a estrutura dos resultados retornados.\n",
    "mscovidbrDf_vacina.printSchema()\n",
    "# Agora estamos prontos para aplicar SQL ou qualquer outro processamento nos resultados\n",
    "# Visualiações:\n",
    "# 1ª - Dos dados de vacinados por UF, município, sexo e idade(média, menor e maior)\n",
    "mscovidbrDf_vacina.select(F.col(\"col._source.paciente_idade\").alias(\"idade\"), \\\n",
    "                    F.col(\"col._source.vacina_nome\").alias(\"vacina\"), \\\n",
    "                    F.col(\"col._source.vacina_descricao_dose\").alias(\"dose\")).show(truncate=False)\n",
    "mscovidbrDf_vacina_sexo_idade = mscovidbrDf_vacina.groupBy(F.col(\"col._source.estabelecimento_uf\").alias(\"UF\"), \\\n",
    "                                                   F.col(\"col._source.estabelecimento_municipio_nome\").alias(\"Municipio\"), \\\n",
    "                                                   F.col(\"col._source.paciente_enumSexoBiologico\").alias(\"Sexo\") \\\n",
    "                                                   ).agg(F.format_number(F.avg(F.col(\"col._source.paciente_idade\")),2).alias(\"Media_de_idade\"), \\\n",
    "                                                       F.min(F.col(\"col._source.paciente_idade\")).alias(\"Idade_menor\"), \\\n",
    "                                                       F.max(F.col(\"col._source.paciente_idade\")).alias(\"Idade_maior\") \\\n",
    "                                                        ).sort(F.asc(\"UF\"))\n",
    "mscovidbrDf_vacina_sexo_idade.show(truncate=False)\n",
    "# Salvando a primeira visualização como tabela Hive\n",
    "mscovidbrDf_vacina_sexo_idade.write.saveAsTable(\"vacina_sexo_idade\")\n",
    "# 2ª - Dos dados de vacinados por UF, município, vacina e dose aplicada\n",
    "mscovidbrDf_vacina_dose = mscovidbrDf_vacina.groupBy(F.col(\"col._source.estabelecimento_uf\").alias(\"UF\"), \\\n",
    "                                                   F.col(\"col._source.estabelecimento_municipio_nome\").alias(\"Municipio\"), \\\n",
    "                                                   F.col(\"col._source.vacina_nome\").alias(\"Vacina\"), \\\n",
    "                                                   F.col(\"col._source.vacina_descricao_dose\").alias(\"Dose\") \\\n",
    "                                                   ).agg(F.count(F.col(\"col._source.vacina_descricao_dose\")).alias(\"Qtd_aplicacoes\") \\\n",
    "                                                        ).sort(F.asc(\"UF\"))\n",
    "mscovidbrDf_vacina_dose.show(truncate=False)\n",
    "# Salvando a segunda visualização com formato parquet e compressão snappy\n",
    "mscovidbrDf_vacina_dose.write.save(\"/user/jes/vacina_parquet\")\n",
    "!hdfs dfs -ls /user/jes/vacina_parquet\n",
    "# 3ª - Dos dados de vacinados por UF, município, raça, sexo, vacina e dose aplicada\n",
    "mscovidbrDf_vacina_dose_sexoraca = mscovidbrDf_vacina.groupBy(F.col(\"col._source.estabelecimento_uf\").alias(\"UF\"), \\\n",
    "                                                   F.col(\"col._source.estabelecimento_municipio_nome\").alias(\"Municipio\"), \\\n",
    "                                                   F.col(\"col._source.vacina_nome\").alias(\"Vacina\"), \\\n",
    "                                                   F.col(\"col._source.vacina_descricao_dose\").alias(\"Dose\"), \\\n",
    "                                                   F.col(\"col._source.paciente_enumSexoBiologico\").alias(\"Sexo\"), \\\n",
    "                                                   F.col(\"col._source.paciente_racaCor_valor\").alias(\"Raca\") \\\n",
    "                                                   ).agg(F.count(F.col(\"col._source.paciente_racaCor_valor\")).alias(\"Qtd_aplicacoes\"), \\\n",
    "                                                         F.min(F.col(\"col._source.paciente_idade\")).alias(\"Idade_menor\"), \\\n",
    "                                                         F.max(F.col(\"col._source.paciente_idade\")).alias(\"Idade_maior\") \\\n",
    "                                                        ).sort(F.asc(\"UF\"))\n",
    "mscovidbrDf_vacina_dose_sexoraca.show()\n",
    "# 4ª - Da quantidade de aplicações por vacina\n",
    "mscovidbrDf_vacina_dose_aplicacoes = mscovidbrDf_vacina.groupBy(F.col(\"col._source.vacina_nome\").alias(\"key\") \\\n",
    "                                                   ).agg(F.count(F.col(\"col._source.vacina_nome\")).cast(\"string\").alias(\"value\") \\\n",
    "                                                        )\n",
    "mscovidbrDf_vacina_dose_aplicacoes.show(truncate=False)\n",
    "mscovidbrDf_vacina_dose_aplicacoes.printSchema()\n",
    "# Salvando a quarta visualização em um tópico no Kafka\n",
    "mscovidbrDf_vacina_dose_aplicacoes.write \\\n",
    "                                  .format(\"kafka\") \\\n",
    "                                  .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "                                  .option(\"topic\", \"topic-kvvacina\") \\\n",
    "                                  .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referência:\n",
    "\n",
    "Neste projeto foi utilizada a biblioteca \"Rest Data Source for Apache Spark\" (https://github.com/sourav-mazumder/Data-Science-Extensions/tree/master/spark-datasource-rest)\n",
    "\n",
    "Esta é uma biblioteca para chamar serviços/APIs baseados em REST para vários conjuntos de parâmetros de entrada em paralelo e agrupar os resultados, retornados pelo serviço REST, em um Dataframe."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
